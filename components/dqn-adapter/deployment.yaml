apiVersion: apps/v1
kind: Deployment
metadata:
  name: dqn-adapter
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dqn-adapter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dqn-adapter
        component: dqn-adapter
      annotations:
        # This annotation is crucial. It tells Alloy/Prometheus to scrape this pod.
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: dqn-adapter-sa
      containers:
      - name: adapter
        # NOTE: You will need to build and push this image to your registry.
        image: nimbusguard-dqn-adapter:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: metrics
        - containerPort: 9091
          name: grpc
        env:
        # OpenAI API key for LLM validation (REQUIRED - system cannot function without it)
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: openai-api-key
              key: key
        # Service URLs
        - name: PROMETHEUS_URL
          value: "http://prometheus.nimbusguard.svc:9090"
        - name: MCP_SERVER_URL
          value: "http://mcp-server.nimbusguard.svc:8080"
        - name: REDIS_URL
          value: "redis://redis.nimbusguard.svc:6379"
        
        # === AI MODEL CONFIGURATION ===
        - name: AI_MODEL
          value: "gpt-4-turbo"  # 128K context window for complex LLM validation
        - name: AI_TEMPERATURE
          value: "0.1"  # Low temperature for consistent reasoning
        
        # === EXPLAINABLE AI CONFIGURATION ===
        - name: ENABLE_DETAILED_REASONING
          value: "true"  # Enable comprehensive AI reasoning logs
        # REMOVED: REASONING_LOG_LEVEL - defined in config but not used in code
        - name: ENABLE_LLM_VALIDATION
          value: "false"     # ENABLED - Use LLM safety validator with MCP tools to override unsafe DQN decisions
        - name: ENABLE_LLM_REWARDS
          value: "true"     # ENABLED - Use LLM for intelligent reward calculation (separate from validation)
        
        # === DQN EXPLORATION STRATEGY ===
        - name: EPSILON_START
          value: "1.0"    # Start with 100% exploration 
        - name: EPSILON_END
          value: "0.05"   # End with 5% exploration
        - name: EPSILON_DECAY
          value: "0.995"  # Decay 0.5% per decision (proper learning curve)
        
        # === DQN TRAINING HYPERPARAMETERS ===
        - name: GAMMA
          value: "0.95"     # Reduced discount factor to focus more on immediate rewards
        - name: LEARNING_RATE
          value: "0.0005"   # Reduced learning rate for more stable convergence
        - name: MEMORY_CAPACITY
          value: "50000"    # Replay buffer size
        - name: MIN_BATCH_SIZE
          value: "4"        # REDUCED - faster learning from negative rewards
        - name: BATCH_SIZE
          value: "8"        # Smaller batches for rapid adaptation to LLM feedback
        - name: TARGET_BATCH_SIZE
          value: "16"       # Smaller target batch for faster Q-value updates
        - name: TARGET_UPDATE_INTERVAL
          value: "500"      # INCREASED - allow target network to stabilize more
        
        # === NEURAL NETWORK ARCHITECTURE ===
        - name: DQN_HIDDEN_DIMS
          value: "64,32"     
        
        
        # === REWARD SYSTEM CONFIGURATION ===
        - name: STABILIZATION_PERIOD_SECONDS
          value: "10"            # Faster decision cycles for more experiences in 30 mins
        
        # === EVALUATION & MONITORING ===
        - name: EVALUATION_INTERVAL
          value: "120"          # More frequent evaluation (2 minutes) for 30-min training
        - name: ENABLE_EVALUATION_OUTPUTS
          value: "true"         # Generate evaluation reports and visualizations
        - name: SAVE_INTERVAL_SECONDS
          value: "120"          # More frequent model saves (2 minutes) for 30-min training
        
        # === DEPLOYMENT CONFIGURATION ===
        - name: TARGET_DEPLOYMENT
          value: "consumer"  # DQN will use ACTUAL resource limits from this deployment's spec
        - name: TARGET_NAMESPACE
          value: "nimbusguard"
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: FORCE_FRESH_MODEL
          value: "true"       # Force using fresh model weights to fix Q-value bias
        - name: CLEAR_EXPERIENCE_BUFFER
          value: "true"       # Clear any existing experience buffer for fresh learning
        readinessProbe:
          exec:
            command:
            - timeout
            - "5"
            - curl
            - "-f"
            - "http://localhost:8080/healthz"
          initialDelaySeconds: 15
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 5
        livenessProbe:
          exec:
            command:
            - timeout
            - "5"
            - curl
            - "-f"
            - "http://localhost:8080/healthz"
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m" 