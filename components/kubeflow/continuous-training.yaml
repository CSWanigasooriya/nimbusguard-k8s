---
# Continuous Training CronJob for DQN Model
apiVersion: batch/v1
kind: CronJob
metadata:
  name: nimbusguard-continuous-training
  namespace: nimbusguard-serving
spec:
  # Run every 4 hours to retrain with new data
  schedule: "0 */4 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          restartPolicy: OnFailure
          containers:
          - name: continuous-trainer
            image: nimbusguard/kubeflow:latest
            command:
            - python
            - /app/continuous_training.py
            env:
            - name: PROMETHEUS_ENDPOINT
              value: "http://prometheus.nimbusguard.svc.cluster.local:9090"
            - name: COLLECTION_HOURS
              value: "4"  # Collect last 4 hours of data
            - name: MODEL_PATH
              value: "/models"
            - name: KSERVE_MODEL_NAME
              value: "nimbusguard-dqn"
            - name: TRAINING_MODE
              value: "continuous"
            - name: MIN_SAMPLES
              value: "100"  # Minimum samples needed for training
            resources:
              requests:
                cpu: 500m
                memory: 1Gi
              limits:
                cpu: 2
                memory: 4Gi
            volumeMounts:
            - name: model-storage
              mountPath: /models
            - name: training-logs
              mountPath: /logs
          volumes:
          - name: model-storage
            persistentVolumeClaim:
              claimName: nimbusguard-model-pvc
          - name: training-logs
            hostPath:
              path: /Users/chamathwanigasooriya/Documents/FYP/nimbusguard/logs
              type: DirectoryOrCreate
---
# ConfigMap with continuous training script
apiVersion: v1
kind: ConfigMap
metadata:
  name: continuous-training-script
  namespace: nimbusguard-serving
data:
  continuous_training.py: |
    #!/usr/bin/env python3
    """
    Continuous Training Script for NimbusGuard DQN
    Collects real metrics and retrains the model with new data
    """
    
    import json
    import logging
    import os
    import time
    from datetime import datetime, timedelta
    from pathlib import Path
    from typing import Dict, List, Any, Tuple
    
    import numpy as np
    import requests
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from collections import deque
    
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('/logs/continuous_training.log')
        ]
    )
    logger = logging.getLogger(__name__)
    
    class ContinuousTrainer:
        def __init__(self):
            self.prometheus_endpoint = os.getenv('PROMETHEUS_ENDPOINT')
            self.collection_hours = int(os.getenv('COLLECTION_HOURS', '4'))
            self.model_path = Path(os.getenv('MODEL_PATH', '/models'))
            self.model_name = os.getenv('KSERVE_MODEL_NAME', 'nimbusguard-dqn')
            self.min_samples = int(os.getenv('MIN_SAMPLES', '100'))
            
            # DQN parameters
            self.state_dim = 11
            self.action_dim = 5
            self.learning_rate = 1e-4
            self.gamma = 0.95
            self.batch_size = 32
            self.epochs = 50
            
            # Initialize model storage
            self.model_dir = self.model_path / self.model_name
            self.model_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"Continuous trainer initialized")
            logger.info(f"Model path: {self.model_dir}")
            logger.info(f"Collection period: {self.collection_hours} hours")
    
        def collect_real_metrics(self) -> List[Dict[str, Any]]:
            """Collect real metrics from Prometheus"""
            logger.info("Collecting real metrics from Prometheus...")
            
            end_time = datetime.now()
            start_time = end_time - timedelta(hours=self.collection_hours)
            
            # Queries for real NimbusGuard metrics
            metrics_queries = {
                'cpu_utilization': 'avg(rate(container_cpu_usage_seconds_total{namespace="nimbusguard"}[5m]))',
                'memory_utilization': 'avg(container_memory_working_set_bytes{namespace="nimbusguard"}) / avg(container_spec_memory_limit_bytes{namespace="nimbusguard"})',
                'network_io_rate': 'avg(rate(container_network_transmit_bytes_total{namespace="nimbusguard"}[5m]))',
                'request_rate': 'avg(rate(http_requests_total{namespace="nimbusguard"}[5m]))',
                'pod_count': 'count(kube_pod_info{namespace="nimbusguard"})',
                'response_time_p95': 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{namespace="nimbusguard"}[5m]))',
                'error_rate': 'avg(rate(http_requests_total{namespace="nimbusguard",status=~"5.."}[5m]))',
                'queue_depth': 'avg(kafka_consumer_lag_sum{namespace="nimbusguard"})',
                'cpu_throttling': 'avg(rate(container_cpu_cfs_throttled_seconds_total{namespace="nimbusguard"}[5m]))',
                'memory_pressure': 'avg(container_memory_working_set_bytes{namespace="nimbusguard"} / container_spec_memory_limit_bytes{namespace="nimbusguard"})',
                'node_utilization': 'avg(1 - rate(node_cpu_seconds_total{mode="idle"}[5m]))',
                # Scaling actions from operator
                'scaling_actions': 'increase(nimbusguard_scaling_events_total[1h])',
                'dqn_decisions': 'increase(nimbusguard_dqn_decisions_total[1h])',
                'current_replicas': 'kube_deployment_status_replicas{namespace="nimbusguard",deployment="consumer-workload"}'
            }
            
            samples = []
            
            # Collect data in 5-minute intervals
            current_time = start_time
            while current_time < end_time:
                timestamp = current_time.timestamp()
                sample = {'timestamp': timestamp}
                
                for metric_name, query in metrics_queries.items():
                    try:
                        # Query Prometheus for this timestamp
                        params = {
                            'query': query,
                            'time': timestamp
                        }
                        response = requests.get(
                            f"{self.prometheus_endpoint}/api/v1/query",
                            params=params,
                            timeout=10
                        )
                        
                        if response.status_code == 200:
                            data = response.json()
                            result = data.get('data', {}).get('result', [])
                            if result:
                                value = float(result[0]['value'][1])
                                sample[metric_name] = value
                            else:
                                sample[metric_name] = 0.0
                        else:
                            logger.warning(f"Failed to query {metric_name}: {response.status_code}")
                            sample[metric_name] = 0.0
                            
                    except Exception as e:
                        logger.warning(f"Error querying {metric_name}: {e}")
                        sample[metric_name] = 0.0
                
                samples.append(sample)
                current_time += timedelta(minutes=5)
                
                # Add small delay to avoid overwhelming Prometheus
                time.sleep(0.1)
            
            logger.info(f"Collected {len(samples)} metric samples")
            return samples
    
        def process_samples_for_training(self, samples: List[Dict[str, Any]]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
            """Process raw samples into DQN training format"""
            logger.info("Processing samples for DQN training...")
            
            states = []
            actions = []
            rewards = []
            
            for i, sample in enumerate(samples[:-1]):  # Exclude last for next_state
                # Create 11-dimensional state vector
                state_vector = [
                    min(max(sample.get('cpu_utilization', 0.0), 0.0), 1.0),
                    min(max(sample.get('memory_utilization', 0.0), 0.0), 1.0),
                    min(max(sample.get('network_io_rate', 0.0) / 1e6, 0.0), 1.0),  # Normalize to MB/s
                    min(max(sample.get('request_rate', 0.0) / 100, 0.0), 1.0),  # Normalize to requests/s
                    min(max(sample.get('pod_count', 1.0) / 10, 0.0), 1.0),  # Normalize assuming max 10 pods
                    min(max(sample.get('response_time_p95', 0.0), 0.0), 1.0),
                    min(max(sample.get('error_rate', 0.0), 0.0), 1.0),
                    min(max(sample.get('queue_depth', 0.0) / 1000, 0.0), 1.0),  # Normalize queue depth
                    min(max(sample.get('cpu_throttling', 0.0), 0.0), 1.0),
                    min(max(sample.get('memory_pressure', 0.0), 0.0), 1.0),
                    min(max(sample.get('node_utilization', 0.0), 0.0), 1.0)
                ]
                
                # Determine action from historical scaling decisions
                # This is simplified - in reality you'd track actual operator decisions
                current_replicas = sample.get('current_replicas', 2)
                next_sample = samples[i + 1]
                next_replicas = next_sample.get('current_replicas', current_replicas)
                
                # Map replica changes to actions
                replica_change = next_replicas - current_replicas
                if replica_change <= -2:
                    action = 0  # SCALE_DOWN_2
                elif replica_change == -1:
                    action = 1  # SCALE_DOWN_1
                elif replica_change == 0:
                    action = 2  # NO_ACTION
                elif replica_change == 1:
                    action = 3  # SCALE_UP_1
                else:  # replica_change >= 2
                    action = 4  # SCALE_UP_2
                
                # Calculate reward based on performance metrics
                reward = self.calculate_reward(sample, next_sample, action)
                
                states.append(state_vector)
                actions.append(action)
                rewards.append(reward)
            
            logger.info(f"Processed {len(states)} training samples")
            logger.info(f"Action distribution: {np.bincount(actions, minlength=5)}")
            logger.info(f"Reward stats: mean={np.mean(rewards):.3f}, std={np.std(rewards):.3f}")
            
            return np.array(states), np.array(actions), np.array(rewards)
    
        def calculate_reward(self, current_sample: Dict, next_sample: Dict, action: int) -> float:
            """Calculate reward for a scaling action"""
            # Performance component
            current_response_time = current_sample.get('response_time_p95', 0.0)
            next_response_time = next_sample.get('response_time_p95', 0.0)
            performance_reward = max(0, current_response_time - next_response_time) * 10
            
            # Efficiency component (penalize unnecessary scaling)
            cpu_util = next_sample.get('cpu_utilization', 0.0)
            memory_util = next_sample.get('memory_utilization', 0.0)
            
            if action == 2:  # NO_ACTION
                efficiency_reward = 0.1  # Small positive for stability
            elif action in [3, 4]:  # SCALE_UP
                efficiency_reward = 0.5 if (cpu_util > 0.7 or memory_util > 0.8) else -0.3
            else:  # SCALE_DOWN
                efficiency_reward = 0.5 if (cpu_util < 0.3 and memory_util < 0.5) else -0.3
            
            # Stability component (penalize errors)
            error_rate = next_sample.get('error_rate', 0.0)
            stability_reward = -error_rate * 5
            
            total_reward = performance_reward + efficiency_reward + stability_reward
            return np.clip(total_reward, -2.0, 2.0)
    
        def load_existing_model(self) -> Tuple[nn.Module, Dict]:
            """Load existing model if available"""
            latest_model_path = self.model_dir / 'latest' / 'model.pth'
            
            if latest_model_path.exists():
                logger.info(f"Loading existing model from {latest_model_path}")
                checkpoint = torch.load(latest_model_path, map_location='cpu')
                
                # Create model
                model = self.create_dqn_model()
                model.load_state_dict(checkpoint['model_state_dict'])
                
                metadata = checkpoint.get('metadata', {})
                logger.info(f"Loaded model trained on {metadata.get('total_samples', 0)} samples")
                return model, metadata
            else:
                logger.info("No existing model found, creating new one")
                return self.create_dqn_model(), {}
    
        def create_dqn_model(self) -> nn.Module:
            """Create DQN model architecture"""
            class DQNModel(nn.Module):
                def __init__(self, state_dim=11, action_dim=5, hidden_dim=128):
                    super().__init__()
                    self.network = nn.Sequential(
                        nn.Linear(state_dim, hidden_dim),
                        nn.ReLU(),
                        nn.LayerNorm(hidden_dim),
                        nn.Dropout(0.1),
                        nn.Linear(hidden_dim, hidden_dim),
                        nn.ReLU(),
                        nn.LayerNorm(hidden_dim),
                        nn.Dropout(0.1),
                        nn.Linear(hidden_dim, action_dim)
                    )
                
                def forward(self, x):
                    return self.network(x)
            
            return DQNModel()
    
        def train_model(self, model: nn.Module, states: np.ndarray, actions: np.ndarray, rewards: np.ndarray, existing_metadata: Dict) -> Dict:
            """Train the DQN model with new data"""
            logger.info("Training DQN model with new data...")
            
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model = model.to(device)
            model.train()
            
            optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)
            criterion = nn.MSELoss()
            
            # Convert to tensors
            states_tensor = torch.FloatTensor(states).to(device)
            actions_tensor = torch.LongTensor(actions).to(device)
            rewards_tensor = torch.FloatTensor(rewards).to(device)
            
            losses = []
            for epoch in range(self.epochs):
                # Sample batch
                batch_size = min(self.batch_size, len(states))
                indices = np.random.choice(len(states), batch_size, replace=False)
                
                batch_states = states_tensor[indices]
                batch_actions = actions_tensor[indices]
                batch_rewards = rewards_tensor[indices]
                
                # Forward pass
                q_values = model(batch_states)
                action_q_values = q_values.gather(1, batch_actions.unsqueeze(1)).squeeze()
                
                # Calculate targets (simplified Q-learning)
                with torch.no_grad():
                    # Use random next states for target calculation (simplified)
                    next_indices = np.random.choice(len(states), batch_size, replace=False)
                    next_states = states_tensor[next_indices]
                    next_q_values = model(next_states).max(1)[0]
                    target_q_values = batch_rewards + self.gamma * next_q_values
                
                loss = criterion(action_q_values, target_q_values)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                losses.append(loss.item())
                
                if epoch % 10 == 0:
                    logger.info(f"Epoch {epoch}/{self.epochs}, Loss: {loss.item():.4f}")
            
            # Calculate training metrics
            final_loss = losses[-1]
            avg_loss = np.mean(losses)
            
            # Update metadata
            new_metadata = {
                'last_training': datetime.now().isoformat(),
                'samples_this_training': len(states),
                'total_samples': existing_metadata.get('total_samples', 0) + len(states),
                'final_loss': final_loss,
                'avg_loss': avg_loss,
                'training_epochs': self.epochs,
                'model_version': existing_metadata.get('model_version', 0) + 1
            }
            
            logger.info(f"Training completed - Final loss: {final_loss:.4f}")
            return new_metadata
    
        def save_model(self, model: nn.Module, metadata: Dict):
            """Save the trained model"""
            # Save to versioned directory
            version = metadata['model_version']
            version_dir = self.model_dir / f'v{version}'
            version_dir.mkdir(exist_ok=True)
            
            # Save model
            model_path = version_dir / 'model.pth'
            torch.save({
                'model_state_dict': model.state_dict(),
                'metadata': metadata,
                'state_dim': self.state_dim,
                'action_dim': self.action_dim
            }, model_path)
            
            # Update latest symlink
            latest_dir = self.model_dir / 'latest'
            if latest_dir.exists():
                latest_dir.unlink()
            latest_dir.symlink_to(f'v{version}')
            
            # Save metadata
            with open(version_dir / 'metadata.json', 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"Model saved to {model_path}")
            logger.info(f"Latest model updated to version {version}")
    
        def trigger_kserve_reload(self):
            """Trigger KServe to reload the updated model"""
            try:
                # In a real implementation, you'd use the KServe API to trigger reload
                # For now, we'll just log that the model is ready
                logger.info("Model updated and ready for KServe to load")
                logger.info("KServe will pick up the new model on next inference request")
                
                # You could also restart the InferenceService here
                # kubectl rollout restart deployment/nimbusguard-dqn-model-predictor -n nimbusguard-serving
                
            except Exception as e:
                logger.error(f"Failed to trigger KServe reload: {e}")
    
        def run_continuous_training(self):
            """Main training loop"""
            try:
                logger.info("Starting continuous training cycle...")
                
                # Collect real metrics
                samples = self.collect_real_metrics()
                
                if len(samples) < self.min_samples:
                    logger.warning(f"Not enough samples ({len(samples)} < {self.min_samples}), skipping training")
                    return
                
                # Process for training
                states, actions, rewards = self.process_samples_for_training(samples)
                
                # Load existing model
                model, existing_metadata = self.load_existing_model()
                
                # Train model
                new_metadata = self.train_model(model, states, actions, rewards, existing_metadata)
                
                # Save updated model
                self.save_model(model, new_metadata)
                
                # Trigger KServe reload
                self.trigger_kserve_reload()
                
                logger.info("Continuous training cycle completed successfully!")
                
            except Exception as e:
                logger.error(f"Continuous training failed: {e}")
                raise
    
    if __name__ == "__main__":
        trainer = ContinuousTrainer()
        trainer.run_continuous_training()
 